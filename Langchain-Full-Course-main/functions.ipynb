{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function calling\n",
    "\n",
    "This new feature proves beneficial in a wide array of situations. It can assist in designing chatbots capable of interacting with various APIs, facilitate task automation, and enable extraction of organized information from inputs expressed in natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade langchain\n",
    "!pip install python-dotenv\n",
    "!pip install openai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you got the AT LEAST Version 0.0.200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "\n",
    "\n",
    "def print_version(package_name):\n",
    "    try:\n",
    "        version = pkg_resources.get_distribution(package_name).version\n",
    "        print(f\"The version of the {package_name} library is {version}.\")\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        print(f\"The {package_name} library is not installed.\")\n",
    "\n",
    "\n",
    "print_version(\"langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to define a function. This one mimics a database query with a \"fake\" price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pizza_info(pizza_name: str):\n",
    "    pizza_info = {\n",
    "        \"name\": pizza_name,\n",
    "        \"price\": \"10.99\",\n",
    "    }\n",
    "    return json.dumps(pizza_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "        \"name\": \"get_pizza_info\",\n",
    "        \"description\": \"Get name and price of a pizza of the restaurant\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"pizza_name\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The name of the pizza, e.g. Salami\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"pizza_name\"],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(query):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-0613\",\n",
    "        messages=[{\"role\": \"user\", \"content\": query}],\n",
    "        functions=functions,\n",
    "    )\n",
    "    message = response[\"choices\"][0][\"message\"]\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(\"What is the capital of france?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How much does pizza salami cost?\"\n",
    "message = chat(query)\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if message.get(\"function_call\"):\n",
    "    function_name = message[\"function_call\"][\"name\"]\n",
    "    pizza_name = json.loads(message[\"function_call\"][\"arguments\"]).get(\"pizza_name\")\n",
    "    print(pizza_name)\n",
    "    function_response = get_pizza_info(\n",
    "        pizza_name=pizza_name\n",
    "    )\n",
    "\n",
    "    second_response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-0613\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "            message,\n",
    "            {\n",
    "                \"role\": \"function\",\n",
    "                \"name\": function_name,\n",
    "                \"content\": function_response,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "second_response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain \n",
    "\n",
    "LangChain allows you to use functions too, but currently (15.06.2023) it is kind of hacky to do it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, AIMessage, ChatMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0613\")\n",
    "message = llm.predict_messages(\n",
    "    [HumanMessage(content=\"What is the capital of france?\")], functions=functions\n",
    ")\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How much does Pizza Salami cost in the restaurant?\"\n",
    "message_pizza = llm.predict_messages([HumanMessage(content=query)], functions=functions)\n",
    "message_pizza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message.additional_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_pizza.additional_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "pizza_name = json.loads(message_pizza.additional_kwargs[\"function_call\"][\"arguments\"]).get(\"pizza_name\")\n",
    "pizza_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pizza_api_response = get_pizza_info(pizza_name=pizza_name)\n",
    "pizza_api_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "second_response = llm.predict_messages(\n",
    "    [\n",
    "        HumanMessage(content=query),\n",
    "        AIMessage(content=str(message_pizza.additional_kwargs)),\n",
    "        ChatMessage(\n",
    "            role=\"function\",\n",
    "            additional_kwargs={\n",
    "                \"name\": message_pizza.additional_kwargs[\"function_call\"][\"name\"]\n",
    "            },\n",
    "            content=pizza_api_response\n",
    "        ),\n",
    "    ],\n",
    "    functions=functions,\n",
    ")\n",
    "second_response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Tools\n",
    "\n",
    "You can convert Tools to functions, both the Tools provided by langchain and also your own tools. The Tool you see is there to show you the interface of a tool, it actually does nothing really do anything useful ;-) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForToolRun,\n",
    "    CallbackManagerForToolRun,\n",
    ")\n",
    "\n",
    "\n",
    "class StupidJokeTool(BaseTool):\n",
    "    name = \"StupidJokeTool\"\n",
    "    description = \"Tool to explain jokes about chickens\"\n",
    "\n",
    "    def _run(\n",
    "        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        return \"It is funny, because AI...\"\n",
    "\n",
    "    async def _arun(\n",
    "        self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"joke tool does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import format_tool_to_openai_function, MoveFileTool\n",
    "\n",
    "\n",
    "tools = [StupidJokeTool(), MoveFileTool()]\n",
    "functions = [format_tool_to_openai_function(t) for t in tools]\n",
    "functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Why does the chicken cross the road? To get to the other side\"\n",
    "output = llm.predict_messages([HumanMessage(content=query)], functions=functions)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = json.loads(output.additional_kwargs[\"function_call\"][\"arguments\"]).get(\"__arg1\")\n",
    "tool_response = tools[0].run(question)\n",
    "tool_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_response = llm.predict_messages(\n",
    "    [\n",
    "        HumanMessage(content=query),\n",
    "        AIMessage(content=str(output.additional_kwargs)),\n",
    "        ChatMessage(\n",
    "            role=\"function\",\n",
    "            additional_kwargs={\n",
    "                \"name\": output.additional_kwargs[\"function_call\"][\"name\"]\n",
    "            },\n",
    "            content=\"\"\"\n",
    "                {tool_response}\n",
    "            \"\"\",\n",
    "        ),\n",
    "    ],\n",
    "    functions=functions,\n",
    ")\n",
    "second_response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Functions Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMMathChain\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")\n",
    "llm_math_chain = LLMMathChain.from_llm(llm=llm, verbose=True)\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Calculator\",\n",
    "        func=llm_math_chain.run,\n",
    "        description=\"useful for when you need to answer questions about math\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(tools, llm, agent=AgentType.OPENAI_FUNCTIONS, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"What is the capital of france?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"What is 100 devided by 25?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
